{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All scripts and information necessary for reproducing the simulations present in this work is available here.**\n",
    "\n",
    "Author: Valeria Añorve-Garibay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate input\n",
    "\n",
    "The length of the simulated region is **437,124 bp** with genetic structure from the modern human genome build GRCh37/hg2019. Defined coordinates: chr12: 40,547,438 - 40,984,562\n",
    "\n",
    "I used the exon ranges defined by the GENCODE v.14 annotations, [Harrow et al. 2012](https://www.gencodegenes.org/human/release_14.html), which can be downloaded from UCSC Genome Browser using GENCODE Genes V14 Basic table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from functools import reduce\n",
    "import os.path, allel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: sim_seq_file\n",
    "# input: a gtf file containing just EXON information/ranges, coords start and coords end are the start\n",
    "# and end of the region to simulate\n",
    "\n",
    "def sim_seq_file(bedFile, coordsStart, coordsEnd, out):\n",
    "    coords = pd.read_csv(bedFile, sep='\\t', header=None)\n",
    "\n",
    "    # exon ranges\n",
    "    exonStarts = np.array(coords[1])[np.array(coords[1]) > coordsStart]\n",
    "    exonEnds = np.array(coords[2])[np.array(coords[2]) > coordsStart]\n",
    "    \n",
    "    # create neutral ranges\n",
    "    # i = 0\n",
    "    neutralStarts = [coordsStart]\n",
    "    neutralEnds = [exonStarts[0]-1]\n",
    "    \n",
    "    i = 1\n",
    "    while i < len(exonStarts):\n",
    "        neutralStarts.append(exonEnds[i-1]+1)\n",
    "        neutralEnds.append(exonStarts[i]-1)\n",
    "        i = i + 1\n",
    "\n",
    "    # long final neutral region\n",
    "    neutralStarts.append(exonEnds[i-1]+1)    \n",
    "    neutralEnds.append(coordsEnd)\n",
    "   \n",
    "    exons = pd.DataFrame({'type': ['exon']*(len(exonStarts)), 'start': exonStarts, 'end': exonEnds})\n",
    "\n",
    "    neutral = pd.DataFrame({'type': ['neutral']*(len(neutralStarts)), 'start': neutralStarts, 'end': neutralEnds})\n",
    "\n",
    "    seq = (pd.concat([exons, neutral], axis=0, ignore_index=True).sort_values(by=['start'])).reset_index(drop=True)\n",
    "    \n",
    "    #seq.to_csv(out_op, sep=' ', index=False)\n",
    "    # create genomic window based on the Neutral and Exon ranges\n",
    "    # i = 0\n",
    "    simStart = 0\n",
    "    simEnd = seq['end'][0]-seq['start'][0]\n",
    "\n",
    "    simStarts = [simStart]\n",
    "    simEnds = [simEnd]\n",
    "\n",
    "    i = 1\n",
    "    while i < len(seq):\n",
    "        simStart = simEnd+1\n",
    "        simStarts.append(simStart)\n",
    "        simEnd = simStart+(seq['end'][i]-seq['start'][i])\n",
    "        simEnds.append(simEnd)\n",
    "        i = i + 1\n",
    "\n",
    "    simSeq = pd.DataFrame({'type': seq['type'], 'start': simStarts, 'end': simEnds})\n",
    "    simSeq.to_csv(out, sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove overlapping/duplicated exons using bedTools:\n",
    "# https://davetang.org/muse/2013/01/18/defining-genomic-regions/\n",
    "# https://github.com/davetang/defining_genomic_regions\n",
    "\n",
    "#awk 'BEGIN{OFS=\"\\t\";} $3==\"exon\" {print $1,$4-1,$5}' MUC19_simRegion.gtf | \\\n",
    "#    bedtools2/bin/sortBed | bedtools2/bin/mergeBed -i - > MUC19_simRegion_exonRanges.gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_seq_file(\"data/MUC19_simRegion_exonRanges.bed\", 40547438, \n",
    "             40984562, \"data/sim_seq_MUC19.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombination map\n",
    "recMap = pd.read_csv(\"data/MUC19_recombMap.bed\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recStart = recMap[2][0] - 40547438\n",
    "recWindow = [recStart]\n",
    "\n",
    "i = 1\n",
    "while i < len(recMap)-1:\n",
    "    recStart = recStart + (recMap[2][i] - recMap[1][i])\n",
    "    recWindow.append(recStart)\n",
    "    i = i + 1\n",
    "\n",
    "recWindow.append(437124)\n",
    "\n",
    "# stdrate must be multiplied by 1e-8 to convert it to cM/bp\n",
    "simRecRates = pd.DataFrame({'type': ['recRate']*len(recWindow), 'pos': recWindow, 'stdrate': recMap[3]})\n",
    "simRecRates.to_csv(\"data/sim_seq_MUC19.rmap\", sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Run SLiM**\n",
    "\n",
    "The SLiM simulation templates can be found in simulations/slim/\n",
    "\n",
    "We obtained 25 simulation replicates for each of the different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population branch statistic (PBS) per SNP\n",
    "# Function: sim_pbs\n",
    "# 1. Find the shared sites between the three samples (A, B and C)\n",
    "# 2. Calculate PBS only in replicates with shared sites\n",
    "\n",
    "# Population branch statistic (PBS) per SNP (combining all SNPs replicates)\n",
    "# Function: sim_pbs\n",
    "# 1. Use the MXL sites as reference\n",
    "# 2. Find the shared sites between the three samples (A, B and C)\n",
    "# 3. Calculate PBS\n",
    "# input:\n",
    "# pathA, pathB and pathC: VCF files\n",
    "# nrep: number of replicates\n",
    "# out: output file name\n",
    "\n",
    "def compute_pbs(pathA, pathB, pathC, nrep, out):\n",
    "    # define dictionaries\n",
    "    pbs_dicc = {}\n",
    "    position_dicc = {}\n",
    "    rep_dicc = {}\n",
    "    pO_dicc = {}\n",
    "    mut_dicc = {}\n",
    "    \n",
    "    rep = 1\n",
    "    \n",
    "    while rep <= nrep:\n",
    "        \n",
    "        # if path exists\n",
    "        path = pathA+str(rep)+'.txt'\n",
    "        \n",
    "        if os.path.isfile(path):\n",
    "            # read vcf files\n",
    "            popA = allel.read_vcf(pathA+str(rep)+'.txt', fields='*')\n",
    "            popB = allel.read_vcf(pathB+str(rep)+'.txt', fields='*')\n",
    "            popC = allel.read_vcf(pathC+str(rep)+'.txt', fields='*')\n",
    "            \n",
    "            popA_multiMask = popA['variants/MULTIALLELIC']\n",
    "            popB_multiMask = popB['variants/MULTIALLELIC']\n",
    "            popC_multiMask = popC['variants/MULTIALLELIC']\n",
    "\n",
    "            popA_pos = popA['variants/POS'] \n",
    "            popB_pos = popB['variants/POS']\n",
    "            popC_pos = popC['variants/POS']\n",
    "\n",
    "            # shared sites between populations\n",
    "            shared_sites = reduce(np.intersect1d, (popA_pos[~popA_multiMask], popB_pos[~popB_multiMask], popC_pos[~popC_multiMask]))\n",
    "            \n",
    "            if shared_sites.size == 0:\n",
    "                print(\"Replicate\", rep, \"does not have shared sites\")\n",
    "                # fill in both dictionaries with Nans\n",
    "                pbs_dicc[rep] = np.array([np.nan])\n",
    "                position_dicc[rep] = np.array([np.nan])\n",
    "                rep_dicc[rep] = np.array([np.nan])\n",
    "                pO_dicc[rep] = np.array([np.nan])\n",
    "                mut_dicc[rep] = np.array([np.nan])\n",
    "            \n",
    "            else:\n",
    "                position_dicc[rep] = shared_sites\n",
    "                rep_dicc[rep] = [rep]*len(shared_sites)\n",
    "                \n",
    "                # shared sites in each population\n",
    "                popA_sites = np.isin(popA_pos, shared_sites)\n",
    "                popB_sites = np.isin(popB_pos, shared_sites)\n",
    "                popC_sites = np.isin(popC_pos, shared_sites)\n",
    "                    \n",
    "                # allele counts\n",
    "                popA_countsArray = allel.GenotypeArray(popA['calldata/GT']).compress(popA_sites, axis=0).count_alleles()\n",
    "                popB_countsArray = allel.GenotypeArray(popB['calldata/GT']).compress(popB_sites, axis=0).count_alleles()\n",
    "                popC_countsArray = allel.GenotypeArray(popC['calldata/GT']).compress(popC_sites, axis=0).count_alleles()\n",
    "                \n",
    "                # calculate the numerator and denominator for Hudson's Fst estimator\n",
    "                popA_popB_num, popA_popB_den = allel.hudson_fst(popA_countsArray, popB_countsArray)\n",
    "                popA_popC_num, popA_popC_den = allel.hudson_fst(popA_countsArray, popC_countsArray)\n",
    "                popC_popB_num, popC_popB_den = allel.hudson_fst(popC_countsArray, popB_countsArray)   \n",
    "                \n",
    "                # calculate Fst\n",
    "                popA_popB_rawFst = popA_popB_num / popA_popB_den\n",
    "                popA_popC_rawFst = popA_popC_num / popA_popC_den\n",
    "                popC_popB_rawFst = popC_popB_num / popC_popB_den\n",
    "                        \n",
    "                # correct for negative Fst values\n",
    "                popA_popB_Fst = np.where(popA_popB_rawFst < 0, 0, popA_popB_rawFst)\n",
    "                popA_popC_Fst = np.where(popA_popC_rawFst < 0, 0, popA_popC_rawFst)\n",
    "                popC_popB_fst = np.where(popC_popB_rawFst < 0, 0, popC_popB_rawFst)\n",
    "                        \n",
    "                # calculate PBS\n",
    "                pbs = (((np.log(1.0 - popA_popB_Fst) * -1.0) +\\\n",
    "                        (np.log(1.0 - popA_popC_Fst) * -1.0) -\\\n",
    "                        (np.log(1.0 - popC_popB_fst) * -1.0)) / float(2))\n",
    "                        \n",
    "                pbs_dicc[rep] = pbs\n",
    "                pOrigin = popA['variants/PO'].compress(popA_sites, axis=0)\n",
    "                pMutType = popA['variants/MT'].compress(popA_sites, axis=0)\n",
    "                pO_dicc[rep] = pOrigin\n",
    "                mut_dicc[rep] = pMutType\n",
    "    \n",
    "        rep = rep + 1\n",
    "    \n",
    "    # concatenate dictionaries to create a data frame\n",
    "    pbs_fullArray = []\n",
    "    position_fullArray = []\n",
    "    rep_fullArray = []\n",
    "    pO_fullArray = []\n",
    "    mutType_fullArray = []\n",
    "\n",
    "    for i in list(pbs_dicc):\n",
    "        pbs_fullArray = np.concatenate((pbs_fullArray, pbs_dicc[i][:]), axis = 0)\n",
    "        position_fullArray = np.concatenate((position_fullArray, position_dicc[i][:]), axis = 0)\n",
    "        rep_fullArray = np.concatenate((rep_fullArray, rep_dicc[i][:]), axis = 0)\n",
    "        pO_fullArray = np.concatenate((pO_fullArray, pO_dicc[i][:]), axis = 0)\n",
    "        mutType_fullArray = np.concatenate((mutType_fullArray, mut_dicc[i][:]), axis = 0)\n",
    "\n",
    "    data = pd.DataFrame({'pbs':pbs_fullArray, 'position': position_fullArray,'pO': pO_fullArray, 'mt': mutType_fullArray, 'rep': rep_fullArray})\n",
    "    data.to_csv(out, sep=' ', index=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform...\n",
    "# Valeria's model\n",
    "vnegative_u = compute_pbs(pathA = 'output/vnegative/uniform/VCF_MXL_negative_',\n",
    "                   pathB = 'output/vnegative/uniform/VCF_EAS_negative_',\n",
    "                   pathC = 'output/vnegative/uniform/VCF_EUR_negative_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_vnegative_uniform.txt\")\n",
    "\n",
    "vneutral_u = compute_pbs(pathA = 'output/vneutral/uniform/VCF_MXL_neutral_',\n",
    "                   pathB = 'output/vneutral/uniform/VCF_EAS_neutral_',\n",
    "                   pathC = 'output/vneutral/uniform/VCF_EUR_neutral_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_vneutral_uniform.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiago's model\n",
    "snegative_u = compute_pbs(pathA = 'output/snegative/uniform/VCF_MXL_snegative_',\n",
    "                   pathB = 'output/snegative/uniform/VCF_EAS_snegative_',\n",
    "                   pathC = 'output/snegative/uniform/VCF_EUR_snegative_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_snegative_uniform.txt\")\n",
    "\n",
    "sneutral_u = compute_pbs(pathA = 'output/sneutral/uniform/VCF_MXL_sneutral_',\n",
    "                   pathB = 'output/sneutral/uniform/VCF_EAS_sneutral_',\n",
    "                   pathC = 'output/sneutral/uniform/VCF_EUR_sneutral_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_sneutral_uniform.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombination map\n",
    "# Valeria's model\n",
    "vnegative_r = compute_pbs(pathA = 'output/vnegative/map/VCF_MXL_negative_',\n",
    "                   pathB = 'output/vnegative/map/VCF_EAS_negative_',\n",
    "                   pathC = 'output/vnegative/map/VCF_EUR_negative_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_vnegative_map.txt\")\n",
    "\n",
    "vneutral_r = compute_pbs(pathA = 'output/vneutral/map/VCF_MXL_neutral_',\n",
    "                   pathB = 'output/vneutral/map/VCF_EAS_neutral_',\n",
    "                   pathC = 'output/vneutral/map/VCF_EUR_neutral_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_vneutral_map.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiago's model\n",
    "snegative_r = compute_pbs(pathA = 'output/snegative/map/VCF_MXL_snegative_',\n",
    "                   pathB = 'output/snegative/map/VCF_EAS_snegative_',\n",
    "                   pathC = 'output/snegative/map/VCF_EUR_snegative_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_snegative_map.txt\")\n",
    "\n",
    "sneutral_r = compute_pbs(pathA = 'output/sneutral/map/VCF_MXL_sneutral_',\n",
    "                   pathB = 'output/sneutral/map/VCF_EAS_sneutral_',\n",
    "                   pathC = 'output/sneutral/map/VCF_EUR_sneutral_', nrep = 25,\n",
    "                   out = \"output/pbs/pbs_sneutral_map.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaag",
   "language": "python",
   "name": "vaag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
